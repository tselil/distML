For testing and evaluation of our optimizer we implemented several variants of the DFC (Divide-Factor-Combine) framework for distributed matrix completion, originally introduced in \cite{Ameet}.

\subsection{DFC Framework Overview}
\subsubsection{Problem Definition}
The DFC framework is used to solve the following well-studied noisy matrix completion problem: Let $A$ be an unknown $m\times n$ matrix of rank $r$, and assume $r<<m,n$. Next $A'$ is obtained by adding a small amount of noise to every entry of $A$. Finally, a small fraction (e.g. $10\%$) of the entries of $A'$ are revealed. The goal is then to find a low-rank matrix $B$ that is a good approximation of the original matrix $A$, using only the revealed entries of $A'$. The standard method of finding such a low rank matrix $B$ is to find a low-rank factorization $B=UV^T$ that agrees with $A'$ on all the revealed entries.

A version of this problem is actually solved in practice by online movie-recommendation systems. Here the matrix has rows corresponding to users and columns corresponding to movies. The assumption that the matrix is low rank corresponds to the assumption that users movie preferences are only based on a relatively small number of factors (e.g. movie genre, movie length, user age etc).

\subsubsection{The DFC Algorithm}
The DFC framework for noisy matrix completion consists of three steps:
\begin{itemize}
\item \textbf{Divide} the input matrix $A$ column-wise into $k$ pieces $\{A_1,A_2,...A_k\}$. More specifically, $A_i$ consists of $n/k$ columns from $A$ and concatenating all the $A_i$ gives the original matrix $A$.
\item \textbf{Factor} each matrix $A_i$ as $U_i V_i^T$ ton a separate processor using some base matrix factorization algorithm.
\item \textbf{Combine} the factorizations $U_iV_i^T$ into one large factorization $UV^T$ using some matrix column projection algorithm.
\end{itemize}
From this description it is clear that there are several significant choices to be made in the implementation of this framework. First, both the base factorization algorithm and the projection algorithm for the combine step must be chosen. Second, the parameter $k$ for determining the number of slices to cut the matrix into has an impact both on runtime and accuracy of the result.

\subsection{Base Factorization Algorithm Choices}
We implemented two base matrix factorization algorithms: Stochastic Gradient Descent (SGD) and Accelerated Proximal Gradient Descent (APG). Both algorithms seek to minimize some objective function which is a combination of the error of the approximation and the rank of the factorization.
\subsubsection{Stochastic Gradient Descent}
Let $S$ be the set of indices for the revealed entries of the noisy matrix $A'$. The objective function for the Stochastic Gradient Descent algorithm is given by:
\[
F(U,V,A') = \sum_{(i,j)\in S} \left(A'_{ij} - (UV^T)_{ij}\right)^2 + \mu (\| U\|^2_F  + \| V\|^2_F)
\]
where $\|U\|_F$ denotes the Frobenius norm of $U$ and $\mu$ is a parameter. Intuitively, the first term in the objective function penalizes errors in matching the revealed entries of $A'$ and the second term is a regularization term to prevent over-fitting. The SGD algorithm uses standard gradient descent techniques to compute $\mbox{argmin}_{U,V} F(U,V,A')$. 

There are two important issues to note about the SGD algorithm. Firstly, the above minimization requires fixed choices for the dimensions of both $U$ and $V$. In particular, this means that we have to first decide on a rank $r$ for our factorization. Then we can run SGD to find a rank $r$ factorization that approximates $A'$ well. The second issue is that the objective function $F(U,V,A')$ is not convex, so there can be local minima where the SGD algorithm gets stuck.

\subsubsection{Accelerated Proximal Gradient Descent}
The objective function for the APG algorithm is given by:
\[
F(U,V,A') = {(i,j)\in S} \left(A'_{ij} - (UV^T)_{ij}\right)^2 + \mu \| UV^T\|_*
\]
where $\|UV^T\|_*$ is the nuclear norm of $UV^T$. The key point here is that the nuclear norm is a convex relaxation of the rank. This means that the above objective in some sense tries to both minimize the error in matching the revealed entries of $A'$, as well as minimize some approximation of the rank of $B=UV^T$.
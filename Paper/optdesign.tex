% !TeX root = 

When designing the optimizer, our goals were to provide a simple 
interface to the user and ensure architecture-independence. To meet the
objective of a simple user interface, the optimizer must be able to
make parameter choices based only on user-specified time, accuracy, and
monetary budget constraints. To meet
the objective of architecture-independence, the optimizer needs to gather
data about jobs run in its specific architecture context. Below,
we describe the design of the optimizer in pursuit of these goals. 

%------------------------------------------------

\subsection{Optimizer System Context}

In this subsection, we discuss interactions between the optimizer and 
the user and algorithmic infrustructure that together form the system. 

\subsubsection{Interface to User}
The user's interface with the optimizer is simple: the user needs to
provide a problem instance for the job, constraints on the amount of
time and money that the job can spend and on the amount of error
that is acceptable for the output. The user also specifies $X$, the 
parameter to be optimized (currently, $X$ can be total runtime, total
error, or amount of money spent).

The user can also specify whether to run the optimizer in {\em explore 
mode}, a mode suitable for high-variance data applications or if the 
optimizer is in the initial stages of learning a new 
distribution (see Section \ref{sec:explore}). The user can also specify
whether to run the optimizer in {\em estimation mode}, a mode in which
the optimizer augments its data with estimates of runtime and error 
using previous jobs from a different parameter 
subspace (see Section \ref{sec:estimate}). This is for example useful 
if the optimizer has learned many examples of
smaller or larger problem instances, but has relatively sparse data
for instances of a given jobs' size. 

The user also specifies which distribution the data comes from (by
distribution, we simply mean the source of the data, since data from
different sources may behave differently). Currently,
this is accomplished by having the user point the optimizer to the 
source of the learned data. When using the optimizer for the first 
couple of times, the user can input statistics from training on similarly
distributed data. There is also a more cursory mode available, in which
the optimizer runs a subproblem and obtains estimates for a wider 
variety of parameter settings (see Section \ref{sec:nodata}). In this 
mode, the user need not specify the distribution of the data. 

\subsubsection{Interface with Algorithm and Infrastructure}
Our optimizer relies on access to an already implemented distributed 
algorithm. The optimizer outputs the parameters that the algorithm 
should run with; in the current implementation this is limited to 
the number of processors and the number of iterations, although this
can be easily extended to algorithm-specific parameters (i.e. learning
rate). 
The algorithm itself is expected to communicate with the distributed
computation framework, although this too could be altered if necessary.

After the algorithm returns, the optimizer requires information about
the outcome of the job so that it can learn. We want to learn the error
and runtime as a function of the number of iterations and the number
of processors used. Because the input parameters are determined by the
optimizer (and thus already known to it), all the optimizer requires 
from the algorithm's output is the error and the runtime. In principle,
if there is an additional parameter over which one wishes to optimize
(say, for example, testing error in addition to training error),
additional information about the algorithm output must be stored. 

%------------------------------------------------

\subsection{Learning}

Our optimizer's parameter selection strategy is based on statistical 
data from prior runs. 
To ensure that the optimizer's choices are architecture-independent,
and to allow the optimizer to adapt to data from different distributions 
and to continually improve its predictions, our optimizer learns from
every job that it encounters. 

This amounts to recording each job's distribution (the actual probability
distribution from which the data comes does not have to be specified--this
is simply a tag, such as ``movielens data'', since we expect different 
kinds of input to have different rates of convergence, etc.), runtime,
error, and the algorithm parameters with which it was run. If there is
another value which one wishes to optimize over, such as training vs. 
testing error, this is information which should be stored as well. 

Additionally, it is possible to seed the optimizer with some manually
constructed training data. If the user already has information about 
previous job runs, or if the user perefers to populate the database
with some systematic parameter setting tests, the optimizer can learn
from these as well. 

%------------------------------------------------

\subsection{Choosing Parameters}

In this subsection, we discuss the way in which our optimizer utilizes
its data to select parameters, and the way in which it obtains information
in the absence of data. 

\subsubsection{Choosing Parameters using Learned Data}
When given a problem instance with a particular budget, our optimizer
consults data from prior runs in order to select the algorithm 
parameters.

What our optimizer does is it scans over all job instances
from the same distribution. If {\em estimation mode} is enabled, 
the optimizer
considers all previous similarly distributed jobs regardless of size, 
making estimates of error and runtime expected for an instance 
of the current size. If estimation mode is not enabled, then the optimizer
discards information about all previous jobs which are not within a
factor of 2 of its size--here, size is determined not by the dimensions
of the matrix but rather by the number of entries. The factor of 2 was 
chosen using non-rigorous empirical trials--it may be that this is not
an appropriate choice for all datasets, but in principle it is easy to
learn this parameter as well, and this is one direction for future work.  

After collecting the relevant prior jobs, 
the optimizer takes an average over values of
time, error and money for identical parameter settings, 
and then chooses the settings that minimize the average value of $X$ while
still meeting the budget constraints. Though we use a straightforward
average here, it may make sense to include the option of taking an
exponentially weighted moving average. In the interest of keeping the
interface simple, we did not implement this, though in principle this too 
could be added with very little additional effort. 

\subsubsection{Choosing Parameters in the Absence of Data}
\label{sec:nodata}

Thus far, our described methods of parameter selection rely on a database
of information about previously completed jobs. However, when instances
from a new data distribution are initially run, there is no past data upon
which we can draw.

For this setting, we do the following: first, we populate our database 
with several smaller jobs--this is currently achieved by running the 
algorithm with a couple of parameter settings for very few iterations. 
This strategy can easily be modified as appropriate--for example, if
minimizing error with a large time budget, and a small monetary budget, 
it may make sense even in this population step 
to have larger numbers of iterations, but fewer processors, or for some
algorithms it may make sense to run the optimizer on a well-chosen 
submatrix. Exploring the best strategy for rapidly populating the database
is an interesting direction for future work.

This gives us some very rough, cursory data at relatively 
little cost. We then run the optimizer on the job using this initial data
in {\em estimation mode} and {\em explore mode}.

\subsubsection{Estimation Mode}
\label{sec:estimate}
Under some circumstances, it is natural that only a limited subspace
of parameter settings has been explored. It is natural to try to extend
what we know about that subspace to gain information about other parameter
choices. For example, if we see only
relatively small matrices from a given distribution, we may want to use
our data to select parameters for a larger one; or, if we have been
operating with a tiny monetary budget and suddenly are able to afford
more processors, we may want to use our data for 1 and 2 processor
jobs to choose parameters when more parallelism is available. 

{\em Estimation mode} tells the optimizer to create estimates of error, time to completion, and expenditure as a function of number of 
iterations, number of processors used, and the size of the instance. The 
optimizer can then use these estimates as synthetic data to populate
unknown areas of the parameter space, and thus make better parameter
choices when data is unavailable. 

The way we've chosen to do estimation is to try to fit data from
prior jobs to models for $time$ and $error$ as functions of iterations, 
number of processors, and instance size. 


{\em Error estimation:} From our empirical observations, the error is an exponential function of
the number of iterations, with an additive term which depends on the number
of processors:
\[
err(s,t) = A\cdot s + e^{-B\cdot t}.
\]
where $s$ is the number of processors and $t$ is the number of iterations, 
and $A$ and $B$ are unknown constants. As far as we have been abe to 
discern, the error is not a function of instance size; however, this
may be because of the particular algorithm we chose for our tests. In 
any case, this should vary from algorithm to algorithm.

{\em Runtime estimation:} The time to completion, at least 
in this method, we have observed to be a polynomial function of the 
number of processors:
\[
time(s,t,n) = \frac{t\cdot n}{s^{C}}. % TODO CHECK THIS!!!!
\]
where again $s$ is the number of processors, $t$ is the number of 
iterations, $n$ is the number of revealed entries in the matrix, 
and $C$ is some unknown constant. This is essentially an application of 
Ahmdahl's Law, but for the algorithm we chose to test on (DFC) the
serial time is negligible compared to the parallel time. The model
for the time dependence should also vary with the specific algorithm
in question. 

The way that the optimizer now works is it fits the extant data to
these models, and then estimates the range of parameters which meet the
budgets, and then populates the database with synthetic data 
over the relevant range. Then, the optimizer performs its usual
parameter selection process, choosing the  number of iterations 
and number of processors that optimize $X$ according to these estimates. 
In practice, the quality of our estimates is very dependent on the 
quality and quantity of data we already have 
(see ?). % TODO: get this data here

However, what matters is that the estimates of $X$ for different 
parameter settings are relatively accurate; that is, if the true value 
of $X$ will be better with parameter settings $p_1$ than with parameter
settings $p_2$, then the estimated one should be better as well. Then, 
if the optimizer is simultaneosly run in {\em explore mode}, the quality
of predictions increases as the optimizer encounters more 
instances (see ?). % TODO: get this data here 

\subsubsection{Explore Mode}
\label{sec:explore}
If the optimizer has access to few previous job outcomes, or if the data
comes from a very high-variance distribution, there is a risk that 
suboptimal parameter settings will be chosen again and again, simply 
because there was one very successful run with those settings, or because
the optimal settings are not known and the optimizer's estimated values
of $X$ are too high for other settings.

In order to prevent the optimizer from returning again and again to
these local optima, the optimizer is able to run in {\em explore mode}. 
In {\em explore mode}, randomization is
used to avoid local optima in parameter selection. Say that we have 
parameter settings $p_1, \ldots, p_m$ with estimated optimized values 
$x_1, \ldots, x_m$. Then the optimizer chooses the output parameters $P$ 
to be $p_i$ with probability
\[
\Pr[P = p_i] = \frac{\frac{1}{x_i}}{\sum_{j = 1}^m \frac{1}{x_j}}.
\]
This scheme has the property that 
\[
\frac{x_i}{x_j} = \alpha\qquad \implies \qquad \frac{\Pr[P = p_i]}{\Pr[P = p_j]} = \frac{1}{\alpha},
\]
That is, a 
particular parameter setting is chosen with probability proportional 
to its relative minimization of $X$.

In this way, the randomness guarantees that the optimizer explores the 
parameter space. Thus as the number of jobs encountered grows, 
the optimizer's data about the entire parameter space improves, and the
influence of outliers on parameter choice is subdued. 



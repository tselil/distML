% !TeX root = 

When designing the optimizer, our goals were to provide a simple 
interface to the user and ensure architecture-independence. To meet the
objective of a simple user interface, the optimizer must be able to
make parameter choices based only on user-specified time, accuracy, and
monetary budget constraints. To meet
the objective of architecture-independence, the optimizer needs to gather
data about jobs run in its specific architecture context. Below,
we describe the design of the optimizer in pursuit of these goals. 

\subsection{Optimizer System Context}


\subsubsection{Interface to User}
The user's interface with the optimizer is simple: the user needs to
provide a problem instance for the job, constraints on the amount of
time and money that the job can spend and on the amount of error
that is acceptable for the output. The user also specifies $X$, the 
parameter to be optimized (currently, $X$ can be total runtime, total
error, or amount of money spent).

The user can also specify whether to run the optimizer in {\em explore 
mode}, a mode suitable for high-variance data applications or if the 
optimizer is in the initial stages of learning a new 
distribution (see Section \ref{sec:explore}). 

Also, the user specifies which distribution the data comes from (by
distribution, we simply mean the source of the data, since data from
different sources may behave differently). Currently,
this is accomplished by having the user point the optimizer to the 
source of the learned data. When using the optimizer for the first 
couple of times, the user can input statistics from training on similarly
distributed data. There is also a more cursory mode available, in which
the optimizer runs a subproblem and obtains estimates for a wider 
variety of parameter settings (see Section \ref{sec:nodata}). In this 
mode, the user need not specify the distribution of the data. 

\subsubsection{Interface with Algorithm and Infrastructure}
Our optimizer relies on access to an already implemented distributed 
algorithm. The optimizer outputs the parameters that the algorithm 
should run with; in the current implementation this is limited to 
the number of processors and the number of iterations, although this
can be easily extended to algorithm-specific parameters (i.e. learning
rate). 
The algorithm itself is expected to communicate with the distributed
computation framework, although this too could be altered if necessary.

After the algorithm returns, the optimizer requires information about
the outcome of the job so that it can learn. We want to learn the error
and runtime as a function of the number of iterations and the number
of processors used. Because the input parameters are determined by the
optimizer (and thus already known to it), all the optimizer requires 
from the algorithm's output is the error and the runtime. In principle,
if there is an additional parameter over which one wishes to optimize
(say, for example, testing error in addition to training error),
additional information about the algorithm output must be stored. 

\subsection{Larnin'}

Our optimizer's parameter selection strategy is based on statistical 
data from prior runs. 
To ensure that the optimizer's choices are architecture-independent,
and to allow the optimizer to adapt to data from different distributions 
and to continually improve its predictions, our optimizer learns from
every job that it encounters. 




\subsection{Choosing Parameters}

\subsubsection{Choosing Parameters using Learned Data}
When given a problem instance with a particular budget, our optimizer
consults data from prior runs in order to select the algorithm 
parameters. 

\subsubsection{Choosing Parameters in the Absence of Data}
\label{sec:nodata}

Thus far, our described methods of parameter selection rely on a database
of information about previously completed jobs. However, when instances
from a new data distribution are initially run, there is no past data upon
which we can draw.

For this setting, we do the following: first, we run such a job for some
small number of iterations, in order to get some cursory data (this can
easily be modified as appropriate; we can run several small jobs, as the 
time budget allows). Then, we use this data to create estimates of the 
error, time to completion, and expenditure as a function of number of 
iterations and number of processors used.

From our empirical observations, the error is an exponential function of
the number of iterations, with an additive term which depends on the number
of processors:
\[
err(s,t) = A\cdot s + e^{-B\cdot t}.
\]
where $s$ is the number of processors and $t$ is the number of iterations, 
and $A$ and $B$ are unknown constants. The time to completion, at least 
in this method, we have observed to be a polynomial function of the 
number of processors:
\[
time(s,t) = \frac{t}{s^{C}}.
\]
where again $s$ is the number of processors, $t$ is the number of iterations, and $C$ is some unknown constant. This is essentially an application of 
Ahmdahl's Law, but for the application we chose to test on (DFC) the
serial time is negligible compared to the parallel time.

What we do in this case is try to fit the extant data from the small
cursory runs to these models, and then use these estimates to choose
the number of iterations and number of processors that optimize $X$ 
within the user's budget. In practice, our initial estimates are 
quite poor (see TODO). 

However, what matters is that the estimates of $X$ for different 
parameter settings are relatively accurate; that is, if the true value 
of $X$ will be better with parameter settings $p_1$ than with parameter
settings $p_2$, then the estimated one should be better as well.

\subsubsection{Explore Mode}
\label{sec:explore}
If the optimizer has access to few previous job outcomes, or if the data
comes from a very high-variance distribution, there is a risk that 
suboptimal parameter settings will be chosen again and again, simply 
because there was one very successful run with those settings, or because
the optimal settings are not known and the optimizer's estimated values
of $X$ are too high for other settings.

In order to prevent the optimizer from returning again and again to
these local optima, the optimizer is able to run in {\em explore mode}. 
In {\em explore mode}, randomization is
used to avoid local optima in parameter selection. Say that we have 
parameter settings $p_1, \ldots, p_m$ with estimated optimized values 
$x_1, \ldots, x_m$. Then the optimizer chooses the output parameters $P$ 
to be $p_i$ with probability
\[
\Pr[P = p_i] = \frac{\frac{1}{x_i}}{\sum_{j = 1}^m \frac{1}{x_j}}.
\]
This scheme has the property that if $\frac{x_i}{x_j} = \alpha$, 
then $\frac{\Pr[P = p_i]}{\Pr[P = p_j]} = \frac{1}{\alpha}$--that is, a 
particular parameter setting is chosen with probability proportional 
to its relative minimization of $X$.

In this way, the randomness guarantees that the optimizer explores the 
parameter space. Thus as the number of jobs encountered grows, 
the optimizer's data about the entire parameter space improves, and the
influence of outliers on parameter choice is subdued. 



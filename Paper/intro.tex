% !TeX root = ../AdaptiveSeedingSTOC.tex
With the rising ubiquity of the Internet in modern times, big data companies such as Google or Facebook have been able to gather increasingly massive amounts of data. These companies often want to leverage their huge databases to learn or predict new information. For example, Netflix utilizes user ratings to improve their recommender service and find new and interesting movies for their users. In order to properly process these mountains of information, there has been considerable work on developing parallelized algorithms for various machine learning tasks. For example, Noisy Matrix Completion is a problem that has applications in recommender systems like one that Netflix uses, and a strongly parallel algorithm for this problem called Divide-Factor-Combine was recently developed at UC Berkeley[CITE]. 

Unfortunately, these algorithms require a significant amount of manual tweaking and adjusting of the parameters. Some algorithms are so sensitive to parameter changes that a small change can cause complete garbage to be returned. In addition to parameter choices, for parallel algorithms one has to choose how much to partition the data. On the one hand, too much partitioning can cause the algorithm to lose error guarantees and perform inadequately. On the other hand, too little partitioning can make the computation take an unacceptably long amount of time. The objectives of minimizing both the error and time of the computation are obviously in conflict. Different users may have different utility curves over these domains. Often an operator who is extremely familiar with both the dataset and the algorithm is required in order to achieve optimal results for a particular utility, but these skilled workers can be difficult to find and expensive to hire and hold. For these reasons, it is very desirable to have an automated process that can pick settings that will be reasonable, and hopefully even close to optimal. 

\subsection{Previous Work}
There has been a large amount of work on automating parameter choices of many algorithms, especially for regularization parameters in various regression algorithms[CITE,CITE,CITE]. However, many of these results depend on assuming some underlying structure in the data, and exploiting that in order to provide analytic techniques that compute the optimal or near-optimal choice of parameter. Our system performs optimization on parameters without assuming an underlying model and only uses its own history to estimate optimal parameters. This approach gives worse results than the model approach if the model is chosen very carefully (or luckily), but this is not always possible. 

\subsection{BESTBUY}
In this work we propose BESTBUY, a new optimization layer for choosing parameter settings for machine learning algorithms, in particular for distributed matrix computations. Every job BESTBUY services comes with an input profile specified by the problem, and completing the job generates an output profile. For example, when performing distributed Noisy Matrix Completion, one input profile could be the number of revealed entries in the matrix, the number of data partitions, and the number of iterations in the algorithm. An output profile could include the time taken for the computation the error in the computation. The system uses the input profile to track similar jobs and allows the user to optimize over the output profile. BESTBUY keeps a database of the $(\text{input},\text{output})$ pairs of every job serviced and uses this database to learn a predictive model of how the input profiles map to the output profiles for this algorithm. After learning this model, the user can specify some convex utility function and linear budgets on the output profile, and BESTBUY will find the input profile that optimizes utility subject to the budgets. 

BESTBUY has performed well in our tests, consistently performing as well as the best input profile in the training dataset. Our preliminary implementation currently only supports a single Noisy Matrix Completion algorithm and a restricted class of utility functions and budgets, and generalizing this framework to more machine learning tasks and a wider class of utilities and budgets is the subject of ongoing work. We have tested on both synthetic and real-world data in the form of Gaussian random matrices and the Movielens10M dataset[CITE]. 

